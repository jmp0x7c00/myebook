/* global Vosk */
import HTMLFlipBook from "react-pageflip";
import React, { useState, useRef } from "react";
import bg from './bg.jpg';
import "./App.css";


const PageCover = React.forwardRef(({ title, image, text }, ref) => {
  return (
    <div
      className="cover"
      ref={ref}
      data-density="hard"
      style={{
        // backgroundImage: `url(${bg})`,
        // backgroundSize: "cover",
        // backgroundPosition: "center",
        // position: "relative",
      }}
    >


	<h2 style={{ textAlign: "center" }}>{title}</h2>
								   
      {image && (
        <img
          src={image}
          alt="cover"
          // style={{
          //   maxWidth: "100%",
          //   maxHeight: "100%",
          //   objectFit: "contain",
          //   display: "block",
          //   margin: "0 auto",
          // }}
        />
      )}
      
      {text && (
        <p className="cartoon-text" style={{ whiteSpace: "pre-wrap" }}>
          {text}
        </p>
      )}
    </div>
  );
});


const Page = React.forwardRef(({ number, content, image }, ref) => {
	return (
		<div className="page" ref={ref}>
		<h2>ç¬¬ {number} é¡µ</h2>
		<hr />
		{image && (
			<img
			src={image}
			alt="page"
			style={{ maxWidth: "100%", maxHeight: "300px", marginBottom: "10px" }}
			/>
		)}
		<p className="cartoon-text" style={{ whiteSpace: "pre-wrap" }}>{content}</p>
		</div>
	);
});


function floatTo16BitPCM(float32Array) {
    const buffer = new Int16Array(float32Array.length);
    for (let i = 0; i < float32Array.length; i++) {
        let s = Math.max(-1, Math.min(1, float32Array[i]));
        buffer[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
    }
    return buffer;
}

function mergeFloat32Arrays(chunks) {
  const length = chunks.reduce((acc, val) => acc + val.length, 0);
  const merged = new Float32Array(length);
  let offset = 0;
  for (const chunk of chunks) {
    merged.set(chunk, offset);
    offset += chunk.length;
  }
  return merged;
}



// ğŸ”¹ æ–°å¢ï¼šå°† Float32Array è½¬ä¸º WAV Blob
function encodeWAV(samples, sampleRate = 16000) {
  const buffer = new ArrayBuffer(44 + samples.length * 2);
  const view = new DataView(buffer);

  function writeString(view, offset, string) {
    for (let i = 0; i < string.length; i++) {
      view.setUint8(offset + i, string.charCodeAt(i));
    }
  }

  const write16 = (offset, value) => view.setUint16(offset, value, true);
  const write32 = (offset, value) => view.setUint32(offset, value, true);

  writeString(view, 0, 'RIFF');
  write32(4, 36 + samples.length * 2);
  writeString(view, 8, 'WAVE');
  writeString(view, 12, 'fmt ');
  write32(16, 16);
  write16(20, 1);
  write16(22, 1);
  write32(24, sampleRate);
  write32(28, sampleRate * 2);
  write16(32, 2);
  write16(34, 16);
  writeString(view, 36, 'data');
  write32(40, samples.length * 2);

  let offset = 44;
  for (let i = 0; i < samples.length; i++, offset += 2) {
    const s = Math.max(-1, Math.min(1, samples[i]));
    view.setInt16(offset, s < 0 ? s * 0x8000 : s * 0x7FFF, true);
  }

  return new Blob([view], { type: 'audio/wav' });
}


function MyAlbum() {
	const bookRef = useRef();
	const [currentPage, setCurrentPage] = useState(0);
	const [isListeningLeft, setIsListeningLeft] = useState(false);
	const [isListeningRight, setIsListeningRight] = useState(false);
	const [isModelLoading, setIsModelLoading] = useState(false);
	  // âœ… æ–°å¢ï¼šå½•éŸ³æ•°æ®ç¼“å­˜
  	const recordedChunksRef = useRef([]);  
	const modelRef = useRef(null);  // ğŸ”¹ ä¿å­˜å…¨å±€æ¨¡å‹
	
	// const [pages, setPages] = useState([
	// 	{ text: "ç¬¬ä¸€é¡µå†…å®¹", image: null },
	// 	{ text: "ç¬¬äºŒé¡µå†…å®¹", image: null },
	// 	{ text: "ç¬¬ä¸‰é¡µå†…å®¹", image: null },
	// 	{ text: "ç¬¬å››é¡µå†…å®¹", image: null },
	// ]);

	const [pages, setPages] = useState([
	  { text: "", image: null, isCover: true },
	  { text: "ç¬¬ä¸€é¡µå†…å®¹", image: null },
	  { text: "ç¬¬äºŒé¡µå†…å®¹", image: null },
	  { text: "ç¬¬ä¸‰é¡µå†…å®¹", image: null },
	  { text: "ç¬¬å››é¡µå†…å®¹", image: null },
	]);

	// ç”¨äºä¿å­˜å½“å‰å½•éŸ³çš„ä¸Šä¸‹æ–‡
	const audioCtxRef = useRef(null);
	const micStreamRef = useRef(null);
	const recognizerRef = useRef(null);

	const handleFlip = (e) => {
		setCurrentPage(e.data);
	};

	// âœ… åœæ­¢å½•éŸ³
	const stopRecording = (side) => {
		try {
			if (audioCtxRef.current) {
				audioCtxRef.current.close();
				audioCtxRef.current = null;
			}
			if (micStreamRef.current) {
				micStreamRef.current.getTracks().forEach((track) => track.stop());
				micStreamRef.current = null;
			}
			if (recognizerRef.current) {
				recognizerRef.current.removeEventListener("result", () => {});
				recognizerRef.current = null;
			}

			if (side === "left") setIsListeningLeft(false);
			else setIsListeningRight(false);

			// âœ… å¯¼å‡º WAV æ–‡ä»¶
			if (recordedChunksRef.current.length > 0) {
				const merged = mergeFloat32Arrays(recordedChunksRef.current);
				const wavBlob = encodeWAV(merged, 48000);
				// å°† Blob è½¬ä¸º Base64 å¹¶æ‰“å°
				const reader = new FileReader();
				reader.onloadend = () => {
					const base64data = reader.result.split(",")[1]; // å»æ‰ data:audio/wav;base64,
					console.log("ğŸ”¹ WAV Base64:", base64data);
				};
				reader.readAsDataURL(wavBlob);
				
				// æ¸…ç©ºç¼“å­˜
				recordedChunksRef.current = [];
			}

			console.log("ğŸŸ¥ å½•éŸ³å·²åœæ­¢");
		} catch (err) {
			console.error("åœæ­¢å½•éŸ³å¤±è´¥:", err);
		}
	};

	// âœ… å¯åŠ¨è¯­éŸ³è¯†åˆ«ï¼ˆVoskletï¼‰
	const startSpeechRecognition = async (side) => {
		try {

			// å¦‚æœæ¨¡å‹æ­£åœ¨åŠ è½½
			if (isModelLoading) {
				alert("æ¨¡å‹åŠ è½½ä¸­ï¼Œè¯·ç¨å...");
				return;
			}
		
		    if ((side === "left" && isListeningLeft) || (side === "right" && isListeningRight)) return;
		
		    if (!modelRef.current) {
	            setIsModelLoading(true);
	            modelRef.current = await Vosk.createModel(
	                "https://myebook-1257475696.cos.ap-shanghai.myqcloud.com/vosk-model-small-en-us-0.15.zip"
	            );
	            setIsModelLoading(false);
	            console.log("âœ… æ¨¡å‹åŠ è½½å®Œæˆ");
        	}
			const recognizer = new modelRef.current.KaldiRecognizer(48000);
			recognizer.setWords(true);
		    recognizer.on("result", (message) => {
		        console.log(`Result: ${message.result.text}`);
				const newPages = [...pages];
				// ğŸ”¹ å¯¹å½“å‰é¡µå¯¹è±¡åšä¸€æ¬¡æµ…æ‹·è´ï¼Œé¿å…ä¿®æ”¹åŸå¯¹è±¡å¼•ç”¨
				const current = { ...newPages[currentPage] }; 
				
				let textOld = '';
				if (current && current.text){
					textOld = current.text;
					if (!textOld || textOld === "" || textOld.includes('å†…å®¹')){
						textOld = '';
					} 
				}
				
				let raw = message.result.text.trim();
				if (raw.length > 0) {
				  raw = raw.charAt(0).toUpperCase() + raw.slice(1);
				}
				let textNew = `${raw}. `;
				
				if (textNew !== '. '){
					if (textOld.endsWith(textNew)){
						console.log(`é‡å¤å†…å®¹ï¼š ${textNew}, è¿‡æ»¤æ‰`);
						return;
					}
					textNew = textOld + textNew;
					current.text = textNew;
				
					// ğŸ”¹ æ›´æ–°æ•°ç»„ä¸­çš„å½“å‰é¡µå¯¹è±¡
					newPages[currentPage] = current;
					setPages(newPages);
				}

		    });
		    recognizer.on("partialresult", (message) => {
		        console.log(`Partial result: ${message.result.partial}`);
				
		    });				

			 const mediaStream = await navigator.mediaDevices.getUserMedia({
		        video: false,
		        audio: {
		            echoCancellation: true,
		            noiseSuppression: true,
		            channelCount: 1,
		            sampleRate: 16000
		        },
		    });
		    
		    const audioContext = new AudioContext();

			 await audioContext.resume();
			
		    
	        const source = audioContext.createMediaStreamSource(mediaStream);
	        const processor = audioContext.createScriptProcessor(4096, 1, 1);
	
	        processor.onaudioprocess = (event) => {
	            const audioBuffer = event.inputBuffer; // âœ… ä¿ç•™ AudioBuffer
    			recognizer.acceptWaveform(audioBuffer); // Vosk æ­£ç¡®ç±»å‹

			// âœ… ä¿å­˜éŸ³é¢‘æ•°æ®å—
			  const channelData = audioBuffer.getChannelData(0);
			  recordedChunksRef.current.push(new Float32Array(channelData));
	        };
	
	        source.connect(processor);
	        processor.connect(audioContext.destination);


			// âœ… ä¿å­˜ä¸Šä¸‹æ–‡
	        audioCtxRef.current = audioContext;
	        micStreamRef.current = mediaStream;
	        recognizerRef.current = recognizer;
	
	        // âœ… æ›´æ–°æŒ‰é’®çŠ¶æ€
	        if (side === "left") setIsListeningLeft(true);
	        else setIsListeningRight(true);
	
	        console.log("ğŸŸ© å½•éŸ³å·²å¼€å§‹");
					
		} catch (err) {
			alert(err);
			console.error("Vosklet Error:", err);
			alert("è¯­éŸ³è¯†åˆ«åˆå§‹åŒ–å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹è·¯å¾„æˆ–éº¦å…‹é£æƒé™ã€‚");
		}
	};

	// âœ… ä¸Šä¼ å›¾ç‰‡å‡½æ•°
	const uploadImage = (side, e) => {
		const file = e.target.files[0];
		if (!file) return;
		const reader = new FileReader();
		reader.onload = (event) => {
			const newPages = [...pages];
			newPages[currentPage].image = event.target.result;
			setPages(newPages);
		};
		reader.readAsDataURL(file);
	};

	return (
		<div
		  style={{
		    backgroundColor: "LightCyan",
		    overflow: "hidden",
		  }}
		>


		<div>
		<HTMLFlipBook
		width={550}
		height={650}
		minWidth={315}
		maxWidth={1000}
		minHeight={420}
		maxHeight={1350}
		showCover={true}
		flippingTime={1000}
		style={{ margin: "0 auto" }}
		maxShadowOpacity={0.5}
		className="album-web"
		ref={bookRef}
		onFlip={handleFlip}
		>

		{pages.map((p, i) =>
		    p.isCover ? (
		      <PageCover
		        key={i}
		        title="I am special!"
		        image={p.image}
		        text={p.text}
		      />
		    ) : (
		      <Page key={i} number={i} content={p.text} image={p.image} />
		    )
  		)}
		</HTMLFlipBook>

		<br />

		{/* âœ… å·¦å³è¯­éŸ³è¾“å…¥åŒº + æ–‡ä»¶ä¸Šä¼ åŒº + åˆ é™¤æŒ‰é’® */}
		{currentPage >= 0 && (
		<div className="formContainer" style={{ display: "flex", justifyContent: "center", alignItems: "center", gap: "20px", marginTop: "20px" }}>
		    {/* ä¸Šä¼ æŒ‰é’® */}
		    <button className="btn" style={{ backgroundColor: "#3498db", color: "white", border: "none", padding: "8px 16px", borderRadius: "6px", cursor: "pointer" }} onClick={() => document.getElementById("fileInputLeft").click()}>
		        ä¸Šä¼ å›¾ç‰‡
		    </button>
		    <input id="fileInputLeft" type="file" accept="image/*" style={{ display: "none" }} onChange={(e) => uploadImage("left", e)} />
		
		    {/* å½•éŸ³æŒ‰é’® */}
		    <button className="btn" onClick={() => isListeningLeft ? stopRecording("left") : startSpeechRecognition("left")} style={{
		        backgroundColor: isListeningLeft ? "red" : "lightgreen",
		        color: "white",
		        border: "none",
		        padding: "8px 16px",
		        borderRadius: "6px",
		        cursor: "pointer",
		    }}>
		        {isListeningLeft ? "åœæ­¢å½•éŸ³" : "ğŸ™ï¸ å¼€å§‹å½•éŸ³"}
		    </button>
		
		    {/* ğŸ”¹ åˆ é™¤æŒ‰é’® */}
		    <button className="btn" style={{
		        backgroundColor: "#e74c3c",
		        color: "white",
		        border: "none",
		        padding: "8px 16px",
		        borderRadius: "6px",
		        cursor: "pointer",
		    }} onClick={() => {
		        const newPages = [...pages];
		        const pageIndex = currentPage; // æ³¨æ„ PageCover å ä½
		        if (newPages[pageIndex] && newPages[pageIndex].text) {
		            let sentences = newPages[pageIndex].text.split(".").filter(s => s.trim() !== "");
		            if (sentences.length > 0) {
		                sentences.pop(); // åˆ é™¤æœ€åä¸€å¥
		                newPages[pageIndex].text = sentences.join(". ") + (sentences.length ? "." : "");
		                setPages(newPages);
		            }
		        }
		    }}>
		        åˆ é™¤æœ€åä¸€å¥
		    </button>
		</div>
		)}


		{/* <p style={{ textAlign: "center" }}>å½“å‰é¡µï¼šç¬¬ {currentPage + 1} é¡µ</p> */}
		</div>
		</div>
	);
}

export default MyAlbum;
